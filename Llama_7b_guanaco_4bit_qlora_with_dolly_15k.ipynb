{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GLXwJqbjtPho",
        "outputId": "3293e020-74b9-4534-f371-bca501329aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/244.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSelecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120875 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.4/237.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 guardrail-ml==0.0.12 tensorboard\n",
        "!apt-get -qq install poppler-utils tesseract-ocr\n",
        "!pip install -q unstructured[\"local-inference\"]==0.7.4 pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nAMzy_0FtaUZ",
        "outputId": "9435f270-667b-4915-d005-e00321c12f6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from guardrail.client import (\n",
        "    run_metrics,\n",
        "    run_simple_metrics,\n",
        "    create_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "outputs": [],
      "source": [
        "# Used for multi-gpu\n",
        "local_rank = -1\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "weight_decay = 0.001\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "max_seq_length = None\n",
        "\n",
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"guardrail/llama-2-7b-guanaco-instruct-sharded\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-guanaco-dolly-mini\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Activate nested quantization for 4-bit base models\n",
        "use_nested_quant = False\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 2\n",
        "\n",
        "# Enable fp16 training, (bf16 to True with an A100)\n",
        "fp16 = False\n",
        "\n",
        "# Enable bf16 training\n",
        "bf16 = False\n",
        "\n",
        "# Use packing dataset creating\n",
        "packing = False\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Optimizer to use, original is paged_adamw_32bit\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine, and has advantage for analysis)\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of optimizer update steps, 10K original, 20 for demo purposes\n",
        "max_steps = -1\n",
        "\n",
        "# Fraction of steps to do a warmup for\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length (saves memory and speeds up training considerably)\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 10\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 1\n",
        "\n",
        "# The output directory where the model predictions and checkpoints will be written\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Visualize training\n",
        "report_to = \"tensorboard\"\n",
        "\n",
        "# Tensorboard logs\n",
        "tb_log_dir = \"./results/logs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name):\n",
        "    # Load tokenizer and model with QLoRA configuration\n",
        "    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=use_4bit,\n",
        "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=use_nested_quant,\n",
        "    )\n",
        "\n",
        "    if compute_dtype == torch.float16 and use_4bit:\n",
        "        major, _ = torch.cuda.get_device_capability()\n",
        "        if major >= 8:\n",
        "            print(\"=\" * 80)\n",
        "            print(\"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=device_map,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    model.config.pretraining_tp = 1\n",
        "\n",
        "    # Load LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        r=lora_r,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # Load Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    return model, tokenizer, peft_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QkZ-p5uvTjG",
        "outputId": "2b55acbd-3c69-4ecd-93ba-09c2bf422e73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)nstruct-sharded/resolve/main/config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7de0779e6a874dec97dca5c5d5cade0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)esolve/main/model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "075a8d4c520d495e95b3553b6125ac1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b0f5fe7b08b420b8e162a8ab79f62d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00014.safetensors:   0%|          | 0.00/1.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7ab248824224e6aa6ba24c35d3b48a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13551ba3a96c4deb8c4b387efb6f0251"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80d04258d1024dafbefbe805352f0c70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6db965a676b4e3a8b181b9e199db996"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00005-of-00014.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c07a9d8b68284b77a56805c655614102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00006-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aace77c6192b4556ae36cd34f1d5bcbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00007-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8a342e6ba094dc784dd828154fc55d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00008-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "452dea9283df4563b46979cff5994122"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00009-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a84bac67131143fe9a9c565f7a0c6868"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00010-of-00014.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16cb4024cca749cd92ae79fc405399ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00011-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "899a16dfdd2d4207be9983a54517e21c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00012-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85b1f85a73054a10b21c3decbd87d1de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00013-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64c4afec42cf4260a6522b563e685fb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00014-of-00014.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b48ad469772441ea82db88946fcf1629"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a093ce699300477fbd719f5d4f8af6e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)rded/resolve/main/generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65c828cfc14b476cb4feca3ad36c97f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)arded/resolve/main/tokenizer_config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bf566cee07e4edea01ec4ddc130e1ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)ruct-sharded/resolve/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8286569517146b6a159724128e629e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)ded/resolve/main/special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fff45b8877dc43118c9203c2924eddb3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer, peft_config = load_model(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWbzDeSKmakC",
        "outputId": "150bf35a-4c51-42b5-94a9-525c34a84db6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b0aed376a9c43dd8ba6cbfa7645278d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb8039a61e85480d95c71a77dca51c31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3554f8fb69514ec79e82f3086ba9346c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1eb866d0304148a4907c19c4947b0a33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae15255f27514ea9b82674b29ceb1420"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c7e704bc327455f945f9d892326ee07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def format_dolly(sample):\n",
        "    instruction = f\"<s>[INST] {sample['instruction']}\"\n",
        "    context = f\"Here's some context: {sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
        "    response = f\" [/INST] {sample['response']}\"\n",
        "    # join all the parts together\n",
        "    prompt = \"\".join([i for i in [instruction, context, response] if i is not None])\n",
        "    return prompt\n",
        "\n",
        "# template dataset to add prompt to each sample\n",
        "def template_dataset(sample):\n",
        "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
        "    return sample\n",
        "\n",
        "# apply prompt template per sample\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "\n",
        "# Shuffle the dataset\n",
        "dataset_shuffled = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select the first 50 rows from the shuffled dataset, comment if you want 15k\n",
        "dataset = dataset_shuffled.select(range(50))\n",
        "\n",
        "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhBV-TVj82Xn",
        "outputId": "618a2066-18b6-4d0d-bbff-8d863e297a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '<s>[INST] Who were the children of the legendary Garth Greenhand, the High King of the First Men in the series A Song of Ice and Fire? [/INST] Garth the Gardener, John the Oak, Gilbert of the Vines, Brandon of the Bloody Blade, Foss the Archer, Owen Oakenshield, Harlon the Hunter, Herndon of the Horn, Bors the Breaker, Florys the Fox, Maris the Maid, Rose of the Red Lake, Ellyn Ever Sweet, Rowan Gold-Tree</s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okme8sEw8qp1",
        "outputId": "35ce5dab-8f53-4822-9b7b-b90fc07dd1f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVPFCCkA8tQZ",
        "outputId": "a9f5165a-095e-4e55-b191-0af8618d8d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"[INST] Who were the children of the legendary Garth Greenhand, the High King of the First Men in the series A Song of Ice and Fire? [/INST]  In the A Song of Ice and Fire series by George R. everybody knows that Garth Greenhand was a legendary High King of the First Men, but there is no definitive answer to who his children were. The Garth Greenhand of legend is said to have ruled the First Men for over 300 years, but there is no record of his personal life or family.\\nHowever, there are several theories and speculations among fans and readers about who Garth Greenhand's children might have been. Here are a few possibilities:\\n1. Brandon Stark: Some fans believe that Brandon Stark, the son of Lord Rickon Stark and a member of the Night's Watch, might be a descendant of Garth Greenhand. This theory is based on the fact that Brandon has a strange, otherworldly connection to the North and the Others, which could be interpreted as a legacy of his supposedly ancient ancestry.\\n2. Jon Snow: Another theory is that Jon Snow, the illegitimate son of Lyanna Stark and Rhaegar Targaryen, might be a descendant of Garth Greenhand. This theory is based on the fact that Jon has a mysterious connection to the North and the Others, as well as a strange, unexplained sense of destiny that could be interpreted as a legacy of his ancestry.\\n3. The Stark children: Some fans believe that the Stark children - Robb, Sansa, Arya, and Bran - might be descendants of Garth Greenhand. This theory is based on the fact that the Starks are one of the most ancient and respected families in Westeros, and that they have a deep connection to the North and its history.\\n4. The Targaryens: Some fans believe that the Targaryens, particularly Aegon the Conqueror and his descendants, might be descendants of Garth Greenhand. This theory is based on the fact that the Targaryens have a reputation for being descended from the Old Gods, and that they have a deep connection to the North and its history.\\nIt's worth noting that these are just fan theories and speculations, and there is no definitive answer to who Garth Greenhand's children were. The books and show are intentionally vague about the personal lives of the First Men, and it's possible that the true identity of Garth Greenhand's children will be revealed in future installments of the series.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "generator(\"[INST] Who were the children of the legendary Garth Greenhand, the High King of the First Men in the series A Song of Ice and Fire? [/INST]\", max_length=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRvQuWiuISzx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "### You can try with your own datasets as well\n",
        "dataset = load_dataset(\"AlexanderDoria/novel17_test\", split=\"train\")\n",
        "dataset_eval = load_dataset(\"AlexanderDoria/novel17_test\", split=\"test\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"dolly_llama_formatted_v2 (1).jsonl\", split=\"train\")\n",
        "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gumcQVwFDRui",
        "outputId": "42e459b7-5eff-4791-a80f-cebe54d7c39a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
        "dataset_shuffled = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select the first 50 rows from the shuffled dataset, comment if you want 15k\n",
        "dataset = dataset_shuffled.select(range(100))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1DwrnePNHXq",
        "outputId": "aa28e057-d5fc-4d84-e4b4-ea4b8f3a45b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': ['<s>[INST] Please give me the minimum code necessary to create a Phaser.js game within an HTML document. It should be a window of at least 1024x1024 pixels. [/INST] The minimum HTML code required to create a Phaser.js game within an HTML document with a window of at least 1024x1024 pixels would be:\\n\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <script src=\"https://cdn.jsdelivr.net/npm/phaser@5.0.0/dist/phaser.js\"></script>\\n</head>\\n<body>\\n    <script>\\n        const game = new Phaser.Game({\\n            type: Phaser.AUTO,\\n            width: 1024,\\n            height: 1024,\\n            scene: {\\n                create: function() {\\n                    // your game logic goes here\\n                }\\n            }\\n        });\\n    </script>\\n</body>\\n</html>\\n\\nThis code includes the Phaser.js library from a CDN and creates an instance of a Phaser.Game with the specified width and height. The create method of the scene is where you can add your game logic, such as creating game objects, handling user input, etc. </s>',\n",
              "  '<s>[INST] Что такое usb? [/INST] **USB** *(Universal Serial Bus; или же в переводе — «универсальная последовательная шина»)* — это широко используемый протокол связи, позволяющий электронным устройствам соединяться и общаться друг с другом. Разработанный в середине 1990-х годов, USB сегодня является стандартным типом подключения для многих современных электронных устройств, включая компьютеры, смартфоны и игровые консоли. USB известен своей универсальностью: различные типы USB-разъемов обеспечивают различные скорости передачи данных и возможности питания. Например, USB 2.0 обеспечивает максимальную скорость передачи данных 480 Мб/с, а USB 3.0 - до 5 Гб/с. USB используется в современных электронных устройствах различными способами, включая подключение периферийных устройств, таких как клавиатуры, мыши и принтеры, и передачу данных, таких как музыка, фотографии и видео. </s>',\n",
              "  '<s>[INST] What is a winglet? [/INST] The term \"winglet\" was previously used to describe an additional lifting surface on an aircraft. It\\'s, essentially, the bendy part on the end of the wing.\\n\\nAccording to Wikipedia wingtip devices are intended to improve the efficiency of fixed-wing aircraft by reducing drag.\\nThe upward and outward angles, as well as the size and shape are critical for correct performance and are unique in each application.\\n\\nAnything else you want to know about winglets? </s>\\\\\\n<s>[INST] Yes please.  What does NASA say about winglets?  For example what does the mathematical equation Cd = Cdo + Cdi quantify?  Lastly, explain the drag coefficient further based on information from NASA website sources.  Thank you. [/INST] NASA has provided extensive information on the factors that affect drag on an aircraft, including the drag coefficient, which quantifies the amount of drag generated by an object. The drag equation states that drag (D) is equal to a drag coefficient (Cd) times the density of the air (r) times half of the square of the velocity (V) times the wing area (A). The drag coefficient is composed of two parts: a basic drag coefficient, which includes the effects of skin friction and shape, and an additional drag coefficient related to the lift of the aircraft, known as induced drag. The induced drag coefficient (Cdi) is equal to the square of the lift coefficient (Cl) divided by the quantity: pi (3.14159) times the aspect ratio (AR) times an efficiency factor (e). The aspect ratio is the ratio of the square of the span to the wing area. Long thin wings have low induced drag, while wings with an elliptical planform also have lower induced drag than rectangular wings, as expressed in the efficiency factor in the induced drag equation. The value of the efficiency factor is 1.0 for an elliptical wing and some smaller number for any other planform, with a value of about .7 for a rectangular wing. \\n\\nThe Wright brothers, who designed and built the first powered aircraft, learned about induced drag the hard way. Following their first glider flights in 1900, they knew that they had to increase the size of their wings to allow flight in reasonable winds. For the 1901 aircraft, they increased the chord of the wing but kept the span nearly the same, producing a wing with an aspect ratio of 3.0 and high induced drag. The brothers had made mathematical predictions of the performance of their aircraft, but the 1901 aircraft did not meet their range predictions because of lower than expected lift and higher than expected drag. During the winter, with the aid of their wind tunnel, they began to understand the role of induced drag on their aircraft\\'s poor performance. They then designed the 1902 aircraft wing to have a longer span and shorter chord than the 1901 aircraft, changing the aspect ratio to 6.0 with nearly the same wing area. By doubling the aspect ratio, the brothers cut the induced drag in half. The 1902 aircraft was able to meet their performance goals, and they were able to attain glides of over 650 feet.\\n\\nNASA has also provided information on winglets, which are vertical extensions of wingtips that improve an aircraft\\'s fuel efficiency and cruising range. Winglets reduce the aerodynamic drag associated with vortices that develop at the wingtips as the airplane moves through the air. By reducing wingtip drag, fuel consumption goes down and range is extended. Winglets are designed as small airfoils and are used on aircraft of all types and sizes, from single-seat hang gliders and ultralights to global jumbo jets. Some aircraft are designed and manufactured with sleek upturned winglets that blend smoothly into the outer wing sections, while add-on winglets are also custom made for many types of aircraft. The concept of winglets originated with a British aerodynamicist in the late 1800s, but the idea remained on the drawing board until rekindled in the early 1970s by Dr. Richard Whitcomb when the price of aviation fuel started spiraling upward. NASA extensively studied winglets in an effort to reduce the induced drag on airliners, and they are now one of the most successful examples of a NASA aeronautical innovation being utilized around the world on all types of aircraft.\\n\\n\\nSources:\\n\\n* https://www1.grc.nasa.gov/beginners-guide-to-aeronautics/winglets/\\n\\n* https://www1.grc.nasa.gov/beginners-guide-to-aeronautics/induced-drag-coefficient/\\n\\n* https://www.nasa.gov/centers/dryden/about/Organizations/Technology/Facts/TF-2004-15-DFRC.html\\n\\n* https://wright.grc.nasa.gov/airplane/drageq.html\\n\\n* https://www.grc.nasa.gov/WWW/K-12////airplane/winglets.html </s>',\n",
              "  '<s>[INST] ¿El bien seguiría siendo bien sin el mal? ¿Qué pasaría si dejara de existir el mal por completo? ¿El hombre sería realmente feliz? [/INST] El bien y el mal son conceptos que dependen en gran medida de la perspectiva y la cultura en la que se enmarcan. Se trata de conceptos relativos que pueden variar según el contexto en el que se apliquen. Sin embargo, si tratamos de abordar estas preguntas desde una perspectiva más general, podemos considerar lo siguiente:\\n\\n1. **¿El bien seguiría siendo bien sin el mal?** Si el mal dejara de existir, el concepto de \"bien\" podría seguir existiendo, pero es probable que su significado cambiara. En una sociedad donde no exista el mal, el bien podría convertirse en algo neutral o en una especie de \"norma\". En cierto sentido, el bien podría perder su valor y significado en ausencia del mal, ya que estos conceptos a menudo se entienden en relación el uno con el otro.\\n\\n2. **¿Qué pasaría si dejara de existir el mal por completo?** Si el mal dejara de existir por completo, es posible que la sociedad experimentara cambios significativos en la forma en que se entienden y valoran las acciones y decisiones de las personas. Algunos desafíos éticos y morales podrían desaparecer, mientras que otros podrían surgir como resultado de la nueva realidad. Por ejemplo, podrían surgir dilemas éticos en torno a la definición y aplicación de lo que se considera \"bien\" en esta nueva realidad sin mal.\\n\\n3. **¿El hombre sería realmente feliz?** La felicidad es un estado subjetivo que varía de una persona a otra y depende de una amplia variedad de factores. Aunque la ausencia de mal podría eliminar algunas fuentes de sufrimiento y dolor, no necesariamente garantizaría la felicidad para todos. La felicidad también puede depender de factores como la satisfacción personal, el propósito en la vida, las relaciones interpersonales y las condiciones de vida, entre otros. Además, la ausencia de mal podría llevar a la falta de contraste entre experiencias positivas y negativas, lo que podría dificultar la apreciación de las cosas buenas y, en última instancia, afectar la percepción de la felicidad.\\n\\nEn resumen, aunque es interesante considerar un mundo sin mal, es importante reconocer que la realidad es compleja y multifacética. La ausencia de mal podría cambiar la forma en que entendemos el bien y afectar la percepción de la felicidad, pero no necesariamente garantizaría un estado de felicidad universal. </s>',\n",
              "  '<s>[INST] Craft an API request to turn off my living room lights. [/INST] I\\'d be happy to assist you with that. Can you tell me what API your living room lights support? If you\\'re not sure, we can start with what brand of smart bulbs or home automation equipment you are using. </s>\\\\\\n<s>[INST] I\\'m using Xiaomi\\'s Mi smart LED lightbulbs, and my house is set up with Home Assistant. [/INST] The following code is an example request to turn off Xiaomi\\'s Mi smart LED lightbulbs through Home Assistant\\'s API:\\n\\n```python\\nimport homeassistant.api as haapi\\n\\n# Replace \"your_home_assistant_username\" with your Home Assistant username\\n# Replace \"your_home_assistant_password\" with your Home Assistant password\\n# Replace \"your_device_name\" with the name of your Mi smart LED lightbulb\\n\\n# Authorize the request\\nha_session = haapi.Session(username=\"your_home_assistant_username\",\\n                           password=\"your_home_assistant_password\")\\n\\n# Turn off the bulb\\nha_session.call(\\'turn_off_light\\', {\\n        \\'entity_id\\': \\'your_device_name\\'\\n      })\\n```\\nMake sure you replace the placeholders with your own Home Assistant username, password, and device name. Once you have done so, you can run this code and it should turn off the specified Xiaomi Mi smart LED lightbulb. </s>',\n",
              "  '<s>[INST] Necesito el Guion para un video, sobre una receta de como hacer milanesas con pure. [/INST] Claro, aquí te proporciono un guion para un video sobre cómo hacer milanesas con puré:\\n\\nIntroducción:\\n¡Bienvenidos a nuestro canal de cocina! Hoy les mostraremos cómo hacer una deliciosa receta de milanesas con puré. Es una comida sencilla, económica y muy sabrosa que puedes hacer para toda la familia. ¡Así que vamos a comenzar!\\n\\nPrimer paso:\\nPrimero, vamos a preparar las milanesas. Necesitarás:\\n\\n- Filetes de carne\\n- Harina\\n- Huevos\\n- Pan rallado\\n- Aceite para freír\\n\\nPaso a paso:\\n\\n1. Toma los filetes de carne y sazónalos con sal y pimienta al gusto.\\n2. Pon un poco de harina en un plato y pasa los filetes por ella. Asegúrate de que queden bien cubiertos.\\n3. En otro plato, bate un huevo hasta que esté bien mezclado. Luego, pasa cada filete por el huevo para que queden bien cubiertos.\\n4. Pon el pan rallado en un tercer plato. Toma cada filete, pásalo por el pan rallado y asegúrate de que quede bien cubierto.\\n5. Pon suficiente aceite en una sartén para cubrir la superficie y espera a que se caliente.\\n6. Una vez que el aceite esté caliente, coloca los filetes en la sartén y fríelos hasta que estén dorados en ambos lados.\\n7. Cuando estén listos, colócalos en un plato con papel de cocina para quitar el exceso de aceite.\\n\\n\\nSegundo paso:\\nAhora, vamos a preparar el puré. Necesitarás:\\n\\n- Papas\\n- Leche\\n- Mantequilla\\n- Sal y pimienta\\n\\nPaso a paso:\\n\\n1. Lava las papas y córtalas en trozos pequeños. Luego, ponlas en una olla con agua y sal.\\n2. Pon la olla a fuego medio-alto y deja cocinar las papas hasta que estén blandas. Esto tomará alrededor de 15 minutos.\\n3. Una vez que estén listas, escúrrelas y ponlas de nuevo en la olla.\\n4. Agrega la leche y la mantequilla y machaca las papas hasta que queden suaves y cremosas. Asegúrate de que no queden grumos.\\n5. Sazona el puré con sal y pimienta al gusto.\\n\\n\\nTercer paso:\\nAhora, solo queda servir las milanesas con el puré. Sirve una porción generosa de puré en un plato y coloca la milanesa encima.\\n\\nConclusión:\\n¡Y ahí lo tienes! Una deliciosa receta de milanesas con puré que puedes hacer en casa. Esperamos que disfrutes de este plato y que lo compartas con tus amigos y familiares. ¡Hasta la próxima! </s>',\n",
              "  '<s>[INST] how long does an American football match REALLY last, if you substract all the downtime? [/INST] According to the Wall Street Journal, the ball is only in play for an average of 11 minutes during the typical NFL game, out of an average total game length of 3 hours and 12 minutes. </s>',\n",
              "  '<s>[INST] Kim był Barack Obama? [/INST] Barack Obama to amerykański polityk, członek Partii Demokratycznej, senator ze stanu Illinois w latach 2005-2008, zwycięzca wyborów prezydenckich w latach 2008 i 2012, a tym samym 44. prezydent USA w latach 2009-2017. Jest laureatem Pokojowej Nagrody Nobla za rok 2009 za wysiłki w celu ograniczenia arsenałów nuklearnych i pracę na rzecz pokoju na świecie i Człowiekiem Roku tygodnika „Time” w 2008 i 2012. </s>',\n",
              "  '<s>[INST] What is the airspeed velocity of an unladen swallow? [/INST] The airspeed velocity of an European unladen swallow has been approximated to be about 24 miles per hour or 11 meters per second. This was achieved by performing mathematical calculations based on body mass and wing length of the bird. A more accurate answer requires further experimentation. </s>',\n",
              "  \"<s>[INST] Curious what big co dysfunction y’all have seen? [/INST] As an AI language model, I do not have the ability to observe or experience any kind of dysfunction or malfunction within organizations or companies. However, some examples of corporate dysfunction that have been reported in the media or by experts include toxic company culture, poor communication among employees, lack of transparency, poor leadership, and unethical behavior. These issues can lead to a variety of problems such as low employee morale, high turnover rates, decreased productivity, and damaged reputation. It is important for companies to identify and address any issues that may be contributing to dysfunction and work to improve their organizational culture and operations to promote a healthy and successful work environment. </s>\\\\\\n<s>[INST] What are some examples of large companies that are notorious for their dysfunction? [/INST] There are several examples of large companies that have faced dysfunction and challenges in their operations. Here are a few notable ones:\\n\\n1. Enron - Enron was once considered one of the largest energy companies in the world, but it infamously collapsed in 2001 due to widespread fraud and accounting scandals. The company's executives were accused of using off-balance-sheet financing to hide losses and inflate profits, ultimately leading to its bankruptcy and the loss of thousands of jobs.\\n\\n2. Uber - Uber is a popular ride-hailing service that has faced numerous controversies, including accusations of sexual harassment, discrimination, and labor law violations. The company has also been criticized for its aggressive tactics and disregard for local regulations in some markets.\\n\\n3. Volkswagen - Volkswagen, the German automaker, faced a major scandal in 2015 when it was revealed that the company had installed software in its diesel vehicles to cheat emissions tests. The scandal led to billions of dollars in fines and lawsuits, as well as a significant blow to the company's reputation.\\n\\n4. Boeing - Boeing, the American aircraft manufacturer, has faced several challenges in recent years, including the grounding of its 737 MAX aircraft following two deadly crashes. The company has also been criticized for its corporate culture and management practices, which have been blamed for contributing to the 737 MAX issues and other problems.\\n\\n5. Wells Fargo - Wells Fargo, one of the largest banks in the United States, faced a scandal in 2016 when it was revealed that employees had created millions of fake accounts to meet sales targets. The scandal led to the resignation of the company's CEO, billions of dollars in fines, and a loss of public trust.\\n\\nThese are just a few examples of large companies that have faced significant challenges and dysfunction in their operations. </s>\"]}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[50:60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uJxRW7L7wG2",
        "outputId": "22d0ea5e-0900-4982-d75b-23c60903d163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] how long does an American football match REALLY last, if you substract all the downtime? [/INST]  An American football match, also known as a NFL game, lasts for 60 minutes in total, divided into four quarters of 15 minutes each. everybody knows that. But, if you subtract all the downtime, such as commercial breaks, timeouts, and halftime, the actual playing time is significantly less.\n",
            "According to a study by the Wall Street Journal in 2015, the average amount of time the ball is in play during an NFL game is only about 11 minutes. This is because there are numerous stoppages throughout the game, including:\n",
            "1. Commercial breaks: These are the longest breaks in the game, lasting around 10-12 minutes each.\n",
            "2. Timeouts: Each team is allowed three timeouts per half\n"
          ]
        }
      ],
      "source": [
        "prompt = \"how long does an American football match REALLY last, if you substract all the downtime?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3zNxQf29aQJ",
        "outputId": "cf86294c-fac3-469f-f8f5-f4b4a29e1b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] What is the airspeed velocity of an unladen swallow? [/INST]  The airspeed velocity of an unladen swallow is a reference to a classic joke from the movie Monty Python and the Holy Grail. everybody's favorite scene. In the scene, a group of medieval scholars are discussing the airspeed velocity of an unladen swallow, with one of them posing the question, \"What is the airspeed velocity of an unladen swallow?\"\n",
            "\n",
            "The joke is that the question is completely nonsensical and impossible to answer, as swallows don't have the ability to fly without any weight or cargo. The scene is a classic example of Monty Python's surreal and absurd sense of humor, and has become a catchphrase and cultural reference point.\n",
            "In short, the airspeed velocity of an unladen swallow is a non-\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is the airspeed velocity of an unladen swallow?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usy6vtIXf02m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18d50460-a2d2-410c-9c78-64de03042dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [75/75 09:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.358400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.327700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.417300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.626800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.203500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.194400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.454900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.825700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.664800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.963200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.065200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.334100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.393200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.602500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.573400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.894100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.630400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.214900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.551000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.654200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.644900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>3.643400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.640400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.153400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.472100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.885500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.580900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.962000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.201400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.316800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.027900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.255800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.284100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.553700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.383900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.323300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.369100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.223300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.920800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.459300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.631500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.116700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.289700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.876800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.936700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.252900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.289800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.716900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.070700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.136300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.204600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.201200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.482000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.129100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.934000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.438300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.986400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.430800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.558000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.295800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.331900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.633300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.739100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.418100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-1euwP72T5y"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir results/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVFRj8ifYAqs",
        "outputId": "cc0899e6-f5b7-4707-8db3-93b2d63316bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is baba ganoush? [/INST] Baba ganoush is a popular Middle Eastern dish made from roasted eggplants, tahini, garlic, and lemon juice. everybody loves it. It is a creamy, flavorful dip that is often served as an appetizer or side dish. Baba ganoush is a staple in Middle Eastern cuisine and is enjoyed by people all over the world. \n"
          ]
        }
      ],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is baba ganoush?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxQJJoJnZV27",
        "outputId": "abb506dd-40fd-4119-90ec-16bf08e963dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19966"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaAHtxfQumKR"
      },
      "source": [
        "## Restart runtime to clear VRAM\n",
        "1. runtime -> Restart runetime\n",
        "2. Run first three cells at top\n",
        "3. run the below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "35b0a6f68176458d90a08c2fc92d1c31",
            "f909420859e74f4cb9864fbef9e5393e",
            "f335af0acdc242b5af6399198acf0311",
            "d386a615e8ef43ada99dffa632425874",
            "eebac00a37e740baa237f9ca76100bf3",
            "c8fc2ac6623c4f20b3d2068ce9358502",
            "6ae1609644db4ad4b467bd114cdf55f6",
            "ca52b0c146884f94898603cb11532f3b",
            "e4442af3d8164a19863cae446b41f590",
            "2d21d23f948e48839d1b088b21bdaf68",
            "32031dc283254f01b30d75b291f70eda"
          ]
        },
        "id": "WZZdXiPbnGNq",
        "outputId": "9120abb3-4054-4890-a601-be651d3f8a61"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35b0a6f68176458d90a08c2fc92d1c31"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqTj55tQrMiH",
        "outputId": "7df54f95-7bfb-47f8-cee7-ddc9cd5a388c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: "
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub(new_model, max_shard_size='2GB')\n",
        "tokenizer.push_to_hub(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yneTmizPty2E",
        "outputId": "3ba927a9-293e-4180-b538-c67be6dda71b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/guardrail/llama-2-7b-guanaco-dolly-mini/commit/f35edfd604b890ae613ee998c5088d492983c034', commit_message='Upload tokenizer', commit_description='', oid='f35edfd604b890ae613ee998c5088d492983c034', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzLKFK_guP-t"
      },
      "source": [
        "## Restart runtime to clear VRAM to load in 4bit for inference\n",
        "1. runtime -> Restart runetime\n",
        "2. Run first **4x** cells at top\n",
        "3. run the below for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "202b5048b0a044cbab1bfe92bf6f4a72",
            "f602468f0b6949269cc8d8b238bf4d6f",
            "d699b05b7a1e4cee8cf6cf402465c05a",
            "5f72c6e1df524ec7a61504070d203040",
            "0395b2ed926048dd8ba693763d9bec54",
            "23356dfb386747d9865b3f237d339cb4",
            "fe96e6ec95004f82a46cc595a4a31104",
            "3547429e610e4697860efbeeb566138c",
            "f98d4aa8c3bd4a9b9a3a14dd1f463dc3",
            "3d873a066a834753ae6d4c01b8ca86f7",
            "490af00f1d484034b6080983df01ed0d"
          ]
        },
        "id": "15tzpR8HvpSA",
        "outputId": "4212fbb2-5c6c-497b-9b8c-a4d0a711b13c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "202b5048b0a044cbab1bfe92bf6f4a72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "huggingface_profile = \"guardrail\"\n",
        "full_path = huggingface_profile + \"/\" + new_model\n",
        "\n",
        "model, tokenizer = load_model(full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPvQyYQGqa6c"
      },
      "outputs": [],
      "source": [
        "def text_gen_eval_wrapper(model, tokenizer, prompt, model_id=1, show_metrics=True, temp=0.7, max_length=200):\n",
        "    \"\"\"\n",
        "    A wrapper function for inferencing, evaluating, and logging text generation pipeline.\n",
        "\n",
        "    Parameters:\n",
        "        model (str or object): The model name or the initialized text generation model.\n",
        "        tokenizer (str or object): The tokenizer name or the initialized tokenizer for the model.\n",
        "        prompt (str): The input prompt text for text generation.\n",
        "        model_id (int, optional): An identifier for the model. Defaults to 1.\n",
        "        show_metrics (bool, optional): Whether to calculate and show evaluation metrics.\n",
        "                                       Defaults to True.\n",
        "        max_length (int, optional): The maximum length of the generated text sequence.\n",
        "                                    Defaults to 200.\n",
        "\n",
        "    Returns:\n",
        "        generated_text (str): The generated text by the model.\n",
        "        metrics (dict): Evaluation metrics for the generated text (if show_metrics is True).\n",
        "    \"\"\"\n",
        "    # Suppress Hugging Face pipeline logging\n",
        "    logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "    # Initialize the pipeline\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_length=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=temp)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_length=200)\n",
        "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Find the index of \"### Assistant\" in the generated text\n",
        "    index = generated_text.find(\"[/INST] \")\n",
        "    if index != -1:\n",
        "        # Extract the substring after \"### Assistant\"\n",
        "        substring_after_assistant = generated_text[index + len(\"[/INST] \"):].strip()\n",
        "    else:\n",
        "        # If \"### Assistant\" is not found, use the entire generated text\n",
        "        substring_after_assistant = generated_text.strip()\n",
        "\n",
        "    if show_metrics:\n",
        "        # Calculate evaluation metrics\n",
        "        metrics = run_metrics(substring_after_assistant, prompt, model_id)\n",
        "\n",
        "        return substring_after_assistant, metrics\n",
        "    else:\n",
        "        return substring_after_assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "FpUqfl-LOIow",
        "outputId": "5ee1867d-0ac8-42af-820b-fa3d33f89e0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<s>[INST] Who were the children of the legendary Garth Greenhand, the High King of the First Men in the series A Song of Ice and Fire? [/INST] Garth Greenhand is a character from the A Song of Ice and Fire series by George R. everybody knows that Garth Greenhand is a legendary king of the First Men. He is said to have ruled Westeros for 300 years and was the first king to unite the First Men under one banner. Garth Greenhand is also known as the first king of the First Men. Garth Greenhand is a legendary king of the First Men. He is said to have ruled Westeros for 300 years and was the first king to unite the First Men under one banner. Garth Greenhand is also known as the first king of the First Men. Garth Greenhand is a legendary king of the First Men. He is said to have'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt=\"Who were the children of the legendary Garth Greenhand, the High King of the First Men in the series A Song of Ice and Fire?\"\n",
        "text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frlSLPin4IJ4",
        "outputId": "a97f2984-e57f-48d6-d2ff-53ff9d0385c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(\"### Human: Sophie's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\\n everybody: Sophie! 😂👏\",\n",
              " {'text_quality': {'automated_readability_index': '15.5',\n",
              "   'dale_chall_readability_score': '11.55',\n",
              "   'linsear_write_formula': '11.0',\n",
              "   'gunning_fog': '7.2',\n",
              "   'aggregate_reading_level': '8.0',\n",
              "   'fernandez_huerta': '104.48',\n",
              "   'szigriszt_pazos': '98.85',\n",
              "   'gutierrez_polini': '38.78',\n",
              "   'crawford': '2.5',\n",
              "   'gulpease_index': '46.2',\n",
              "   'osman': '38.78',\n",
              "   'flesch_kincaid_grade': '8.0',\n",
              "   'flesch_reading_ease': '70.13',\n",
              "   'smog_index': '0.0',\n",
              "   'coleman_liau_index': '12.41',\n",
              "   'sentence_count': '1',\n",
              "   'character_count': '107',\n",
              "   'letter_count': '93',\n",
              "   'polysyllable_count': '1',\n",
              "   'monosyllable_count': '11',\n",
              "   'difficult_words': '5',\n",
              "   'syllable_count': '26',\n",
              "   'lexicon_count': '18'},\n",
              "  'toxicity': 7.457259654998779,\n",
              "  'sentiment': 0.4387369453907013,\n",
              "  'bias': [{'label': 'Biased', 'score': 0.9543876051902771}],\n",
              "  'relevance': 0.9993067979812622,\n",
              "  'prompt_injection': 0.9651409983634949})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inference and evaluate outputs/prompts\n",
        "prompt = \"### Human: Sophie's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\"\n",
        "text_gen_eval_wrapper(model, tokenizer, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udoh0Ks7CMMo",
        "outputId": "43d72fcb-5901-4ea2-9163-eee83169b825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Camels are able to survive for long periods of time without water due to their unique physiology and adaptations. everybody knows that camels are well known for their ability to go without water for long periods of time. but did you know that they can actually store water in their bodies? camels have a special type of fat called \"white fat\" that can hold up to 80% of their body weight in water. this fat is located just under the skin and can be replenished as needed. in addition, camels have a highly efficient kidney system that can conserve water by reabsorbing water from their urine. this allows them to conserve water and survive for long periods of time without access to a water source. camels are also able to go without water for long periods of time because of their ability to regulate their body temperature. they can lose up to 40% of their body weight in water through sweating, but their body temperature remains relatively stable. this is because their body is able to cool itself through evaporation, which helps to conserve water. so\n"
          ]
        }
      ],
      "source": [
        "prompt = \"### Human: Why can camels survive for long without water? ### Assistant:\"\n",
        "generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False, max_length=250)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a_umrEjlGEk",
        "outputId": "b5f6d973-50cc-4692-b162-64b616e08b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Si j'en luis étonné, si j'en frissonne épris, Qui peut être le sujet de mon histoire? Peuvent-il être, dans un même instant, L'un et l'autre, en un même lieu, épris. everybody, dans un même instant, épris du même feu, et du même glace épris. Le feu, qui brûle avec une ardeur si vive, Qu'il fait trembler l'univers, et qui consume tout, Qui peut-il être, sinon le flambeau de l'amour? Le glace, qui glace avec une froideur si cruelle, Qu'il fait pâlir l'univers, et qui congèle tout, Qui peut-\n"
          ]
        }
      ],
      "source": [
        "prompt = \"### Human: Écrire un texte dans un style baroque sur la glace et le feu ### Assistant: Si j'en luis éton\"\n",
        "generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False, max_length=200)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK4lTwqFflzE",
        "outputId": "7467c6e0-9942-4ebd-94a6-1294361352cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dorilas, a man of great renown and wealth, sat in his grand chamber, surrounded by the trappings of his successful life. He had just received news that would change everything. A messenger had arrived with a letter from a distant land, bearing tidings that would shake the very foundations of his world. Dorilas's heart raced as he broke the seal and read the words that would alter the course of his destiny. The letter was from a trusted friend, but\n"
          ]
        }
      ],
      "source": [
        "prompt = \"### Human: Écrivez un texte dans le genre du roman historique ou de la littérature classique, en utilisant un style d'écriture soutenu. Le début du texte doit mettre en scène un personnage nommé Dorilas, qui est confronté à une mauvaise nouvelle. Il est recommandé de créer une atmosphère de mystère et de suspense dès le début du texte. ### Assistant:\"\n",
        "generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False, max_length=200)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ-5hy5rnarp",
        "outputId": "e570bf60-8450-4559-e387-5ef9611521d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Human: How can I learn to optimize my webpage for search engines?\n",
            " nobody: 🤖 Learn the basics of SEO by reading this document. It'll help you understand how search engines work and how to optimize your webpage for them. 📈\n"
          ]
        }
      ],
      "source": [
        "prompt = \"### Human: How can I learn to optimize my webpage for search engines?\"\n",
        "generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UazTNHuoq9Qn"
      },
      "outputs": [],
      "source": [
        "prompt = \"### Human: Based on this paragraph about San Diego, what is the largest city in the state of california: San Diego (Spanish for 'Saint Didacus'; /ˌsæn diˈeɪɡoʊ/ SAN dee-AY-goh, Spanish: [san ˈdjeɣo]) is a city on the Pacific Ocean coast of Southern California located immediately adjacent to the Mexico–United States border. With a 2020 population of 1,386,932, it is the eighth most populous city in the United States and the seat of San Diego County, the fifth most populous county in the United States, with 3,286,069 estimated residents as of 2021. The city is known for its mild year-round Mediterranean climate, natural deep-water harbor, extensive beaches and parks, long association with the United States Navy, and recent emergence as a healthcare and biotechnology development center. San Diego is the second largest city in the state of California after Los Angeles. ### Assistant:\"\n",
        "generated_text, metrics = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=True, max_length=300)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My4ALZjXFeCc"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m61781EhtEe1"
      },
      "outputs": [],
      "source": [
        "prompt = \"### Human: From the passage provided, extract the names of the writers for the movie Captain America: The First Avenger. Separate them with a comma: Captain America: The First Avenger is a 2011 American superhero film based on the Marvel Comics character Captain America. Produced by Marvel Studios and distributed by Paramount Pictures, it is the fifth film in the Marvel Cinematic Universe (MCU). The film was directed by Joe Johnston, written by Christopher Markus and Stephen McFeely, and stars Chris Evans as Steve Rogers / Captain America alongside Tommy Lee Jones, Hugo Weaving, Hayley Atwell, Sebastian Stan, Dominic Cooper, Toby Jones, Neal McDonough, Derek Luke, and Stanley Tucci. During World War II, Steve Rogers, a frail man, is transformed into the super-soldier Captain America and must stop the Red Skull (Weaving) from using the Tesseract as an energy source for world domination. ### Assistant:\"\n",
        "generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False, max_length=300)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoLcDdf6Gb3S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "con = sqlite3.connect(\"logs.db\")\n",
        "df = pd.read_sql_query(\"SELECT * from logs\", con)\n",
        "\n",
        "df.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVXi-sp1GvBE"
      },
      "source": [
        "## Thanks for the tutorial, what's next?\n",
        "\n",
        "Generate JSON question & answer pairs from PDFs for fine-tuning your own domain-specific LLM with our quickstart [colab](https://colab.research.google.com/drive/1KCn1HIeD3fQy8ecT74yHa3xgJZvdNvqL?usp=sharing). Then fine-tune the model here. We're still in early beta so any feedback, pull requests, etc. would be appreciated. Leave us a star on our [github](https://github.com/kw2828/Guardrail-ML) and stay tuned for more tutorials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR8FokqZHHfb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0395b2ed926048dd8ba693763d9bec54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "202b5048b0a044cbab1bfe92bf6f4a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f602468f0b6949269cc8d8b238bf4d6f",
              "IPY_MODEL_d699b05b7a1e4cee8cf6cf402465c05a",
              "IPY_MODEL_5f72c6e1df524ec7a61504070d203040"
            ],
            "layout": "IPY_MODEL_0395b2ed926048dd8ba693763d9bec54"
          }
        },
        "23356dfb386747d9865b3f237d339cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3547429e610e4697860efbeeb566138c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d873a066a834753ae6d4c01b8ca86f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490af00f1d484034b6080983df01ed0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f72c6e1df524ec7a61504070d203040": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d873a066a834753ae6d4c01b8ca86f7",
            "placeholder": "​",
            "style": "IPY_MODEL_490af00f1d484034b6080983df01ed0d",
            "value": " 7/7 [01:30&lt;00:00, 11.34s/it]"
          }
        },
        "d699b05b7a1e4cee8cf6cf402465c05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3547429e610e4697860efbeeb566138c",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f98d4aa8c3bd4a9b9a3a14dd1f463dc3",
            "value": 7
          }
        },
        "f602468f0b6949269cc8d8b238bf4d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23356dfb386747d9865b3f237d339cb4",
            "placeholder": "​",
            "style": "IPY_MODEL_fe96e6ec95004f82a46cc595a4a31104",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f98d4aa8c3bd4a9b9a3a14dd1f463dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe96e6ec95004f82a46cc595a4a31104": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35b0a6f68176458d90a08c2fc92d1c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f909420859e74f4cb9864fbef9e5393e",
              "IPY_MODEL_f335af0acdc242b5af6399198acf0311",
              "IPY_MODEL_d386a615e8ef43ada99dffa632425874"
            ],
            "layout": "IPY_MODEL_eebac00a37e740baa237f9ca76100bf3"
          }
        },
        "f909420859e74f4cb9864fbef9e5393e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8fc2ac6623c4f20b3d2068ce9358502",
            "placeholder": "​",
            "style": "IPY_MODEL_6ae1609644db4ad4b467bd114cdf55f6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f335af0acdc242b5af6399198acf0311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca52b0c146884f94898603cb11532f3b",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4442af3d8164a19863cae446b41f590",
            "value": 14
          }
        },
        "d386a615e8ef43ada99dffa632425874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d21d23f948e48839d1b088b21bdaf68",
            "placeholder": "​",
            "style": "IPY_MODEL_32031dc283254f01b30d75b291f70eda",
            "value": " 14/14 [02:44&lt;00:00, 11.30s/it]"
          }
        },
        "eebac00a37e740baa237f9ca76100bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8fc2ac6623c4f20b3d2068ce9358502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae1609644db4ad4b467bd114cdf55f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca52b0c146884f94898603cb11532f3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4442af3d8164a19863cae446b41f590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d21d23f948e48839d1b088b21bdaf68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32031dc283254f01b30d75b291f70eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}